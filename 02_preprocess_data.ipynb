{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initial Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from scipy import sparse, io\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "RAW_TWEETS_PATH = \"/data/dnc2020/raw_tweets/\"\n",
    "PARQUET_TWEET_PATH = '/data/navid/processed_tweets'\n",
    "PARQUET_USER_PATH = \"/data/navid/processed_users\"\n",
    "PARQUET_RTNET_PATH = \"/data/kenny/processed_rt_network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We use spark to analyze the data efficiently. You may opt for a different format\n",
    "## Also note we here assume a machine with a reasonably large number of cores (16) \n",
    "## and RAM (at least 32 GB). Adjust below accordingly\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[{}]\".format(16))\n",
    "    .config(\"spark.driver.memory\", \"{}g\".format(50))\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{10}g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Move data from raw to processed\n",
    "\n",
    "Generate processed tweets, users, RT graph (easier loading/analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import run_data_prep, create_retweet_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path /data/navid/processed_tweets already exists, assuming parquet has been generated already\n"
     ]
    }
   ],
   "source": [
    "# Processed tweet format\n",
    "if not os.path.exists(PARQUET_TWEET_PATH):\n",
    "    run_data_prep(stream_path_template=os.path.join(RAW_TWEETS_PATH,\"{date}*/*.gz\"),\n",
    "                         output_path_template=os.path.join(PARQUET_TWEET_PATH,\"processed_tweets_{date}/\"),\n",
    "                         begin_date=\"2020-01-01\",\n",
    "                         end_date=\"2020-12-31\",\n",
    "                         batch_size=100000,\n",
    "                         mode='tweet'\n",
    "    )\n",
    "else:\n",
    "    print(f\"path {PARQUET_TWEET_PATH} already exists, assuming parquet has been generated already\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path /data/navid/processed_users already exists, assuming parquet has been generated already\n"
     ]
    }
   ],
   "source": [
    "# Processed user format\n",
    "if not os.path.exists(PARQUET_USER_PATH):\n",
    "    run_data_prep(stream_path_template=os.path.join(RAW_TWEETS_PATH,\"{date}*/*.gz\"),\n",
    "                         output_path_template=os.path.join(PARQUET_TWEET_PATH,\"processed_users_{date}/\"),\n",
    "                         begin_date=\"2020-01-01\",\n",
    "                         end_date=\"2020-12-31\",\n",
    "                         batch_size=100000,\n",
    "                         mode='user'\n",
    "    )\n",
    "else:\n",
    "    print(f\"path {PARQUET_USER_PATH} already exists, assuming parquet has been generated already\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path /data/kenny/processed_rt_network already exists, assuming parquet has been generated already\n"
     ]
    }
   ],
   "source": [
    "# Processed retweet graph \n",
    "if not os.path.exists(PARQUET_RTNET_PATH):\n",
    "    spark = (\n",
    "            SparkSession\n",
    "            .builder\n",
    "            .master(\"local[{}]\".format(16))\n",
    "            .config(\"spark.driver.memory\", \"{}g\".format(50))\n",
    "            .config(\"spark.driver.maxResultSize\", f\"{10}g\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "    print(f\"Creating RT graph at {PARQUET_RTNET_PATH}\")\n",
    "    create_retweet_graph(spark,\n",
    "                         output_path=PARQUET_RTNET_PATH, \n",
    "                         all_tweets_path=PARQUET_TWEET_PATH)\n",
    "    spark.stop()\n",
    "else:\n",
    "    print(f\"path {PARQUET_RTNET_PATH} already exists, assuming parquet has been generated already\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Read in data and get some basic numbers for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_tweets_df = spark.read.parquet(os.path.join(PARQUET_TWEET_PATH, '*', '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'N total tweets: 589729860  N unique users: [Row(count(DISTINCT uid)=20928178)] N non-retweets: 169614594'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tweets = all_tweets_df.count()\n",
    "n_unique_users = all_tweets_df.select(F.countDistinct(\"uid\")).collect()\n",
    "non_rt_cnt = all_tweets_df.filter(F.col(\"rt_id\").isNull()).count()\n",
    "\n",
    "f\"N total tweets: {n_tweets}  N unique users: {n_unique_users} N non-retweets: {non_rt_cnt}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data for Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data for Figure 1\n",
    "from datetime import date,timedelta\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "start_date = date(2020, 1, 1)\n",
    "end_date = date(2020, 12, 31)\n",
    "dat = []\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    \n",
    "    datestr= single_date.strftime(\"%Y-%m-%d\")\n",
    "    filpath = os.path.join(PARQUET_TWEET_PATH, \n",
    "                                        f'processed_tweets_{datestr}', \n",
    "                                        '*')\n",
    "    g = glob.glob(filpath)\n",
    "    #print(datestr,filpath,len(g))\n",
    "    if not len(g):\n",
    "        dat.append((datestr,0))\n",
    "    else:\n",
    "        p = spark.read.parquet(filpath)\n",
    "        z = p.withColumn(\"date\",F.from_unixtime(F.col(\"created_at\"),\"yyyy-MM-dd\")).groupby(\"date\").count().collect()\n",
    "        dat += [(x[0], x[1]) for x in z]\n",
    "pd.DataFrame(dat, columns=['date','count']).to_csv(\"data/tweet_counts.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute fraction of data from top retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================================>       (172 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tweets RT'd >10K times:  4355\n",
      "% of all tweets:  0.14926089548865645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_tweets_df = spark.read.parquet(os.path.join(PARQUET_TWEET_PATH, '*', '*'))\n",
    "\n",
    "rt_counts = (all_tweets_df.filter(F.col(\"rt_id\").isNotNull())\n",
    "             .groupby(\"rt_id\")\n",
    "             .agg(F.count(F.lit(1)).alias('rt_times'))\n",
    "             .filter(F.col(\"rt_times\") >= 10000)\n",
    "            )\n",
    "rt_pd = rt_counts.toPandas()\n",
    "print(\"N tweets RT'd >10K times: \", len(rt_pd))\n",
    "print(\"% of all tweets: \",rt_pd.rt_times.sum()/589729860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Stats on RT network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+--------+\n",
      "|uid                |rt_uid             |rt_times|\n",
      "+-------------------+-------------------+--------+\n",
      "|350383251          |1122623265673891840|17      |\n",
      "|14940699           |1177630382084124672|1       |\n",
      "|243795934          |1177630382084124672|1       |\n",
      "|2675291408         |1134899032348712960|12      |\n",
      "|1173361710477934598|1159600736021504006|13      |\n",
      "+-------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rtd_users = spark.read.parquet(PARQUET_RTNET_PATH)\n",
    "rtd_users.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N edges in RT net:  166021415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N users in RT net:  14244949\n",
      "Percent of all users sending >=1 RT: 0.6806588227603951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:=======================================>                (14 + 6) / 20]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total N rts [Row(sum(rt_times)=431081836)]\n",
      "% of all tweets that are RTs:  0.7309818702414017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rtd_users_cnt = rtd_users.groupby(\"uid\").count()\n",
    "print(\"N edges in RT net: \", rtd_users.count())\n",
    "print(\"N users in RT net: \", rtd_users_cnt.count())\n",
    "print(\"Percent of all users sending >=1 RT:\", 14244949/20928178)\n",
    "print(\"Total N rts\", rtd_users.agg({'rt_times':\"sum\"}).collect())\n",
    "print(\"% of all tweets that are RTs: \", 431081836/589729860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out all users who have RT'd at least once and have a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_df = (spark.read.parquet(os.path.join(PARQUET_USER_PATH, '*', '*'))[\n",
    "                                F.col(\"description\").isNotNull()].\n",
    "                                            dropDuplicates(subset=['id','description'])\n",
    "               )\n",
    "print(\"N users w/ description: \", all_users_df.select(F.countDistinct('id')).collect())\n",
    "print(\"% with description:\", 15953966/20928178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtd_user_descript = rtd_users_cnt.join(all_users_df, rtd_users_cnt.uid == all_users_df.id)\n",
    "rtd_user_descript.write.parquet(\"/data/dnc2020/user_descript\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 Create Trimmed Retweet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_THRESHOLD = 12\n",
    "RTD_USER_THRESHOLD = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------+\n",
      "|uid      |rt_uid             |rt_times|\n",
      "+---------+-------------------+--------+\n",
      "|350383251|1122623265673891840|17      |\n",
      "|14940699 |1177630382084124672|1       |\n",
      "+---------+-------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rt_net = spark.read.parquet(PARQUET_RTNET_PATH)\n",
    "rt_net.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefiltering:\n",
      "\n",
      "Users: [Row(count(uid)=14244949)]\n",
      "RT'd accounts: [Row(count(rt_uid)=2308610)]\n",
      "Total Links: 166021415\n",
      "Total RTs: [Row(sum(rt_times)=431081836)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = rt_net.agg(F.countDistinct(F.col(\"uid\"))).collect()\n",
    "b = rt_net.agg(F.countDistinct(F.col(\"rt_uid\"))).collect()\n",
    "c = rt_net.count()\n",
    "d = rt_net.agg({\"rt_times\" : \"sum\"}).collect()\n",
    "\n",
    "print(f\"\"\"Prefiltering:\n",
    "\n",
    "Users: {a}\n",
    "RT'd accounts: {b}\n",
    "Total Links: {c}\n",
    "Total RTs: {d}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = rt_net.groupby(\"uid\").count().filter(F.col(\"count\") >= USER_THRESHOLD)\n",
    "rtd_counts = rt_net.groupby(\"rt_uid\").count().filter(F.col(\"count\") >= RTD_USER_THRESHOLD)\n",
    "\n",
    "rt_net = rt_net.join(user_counts.select(\"uid\"), [\"uid\"])\n",
    "rt_net = rt_net.join(rtd_counts.select(\"rt_uid\"), [\"rt_uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rt_net.write.parquet(\"data/trimmed_rtnet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_net = spark.read.parquet(\"data/trimmed_rtnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post filtering:\n",
      "\n",
      "Users: [Row(count(uid)=1791912)]\n",
      "RT'd accounts: [Row(count(rt_uid)=25395)]\n",
      "Total Links: 111840107\n",
      "Total RTs: [Row(sum(rt_times)=325107353)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = trimmed_net.agg(F.countDistinct(F.col(\"uid\"))).collect()\n",
    "b = trimmed_net.agg(F.countDistinct(F.col(\"rt_uid\"))).collect()\n",
    "c = trimmed_net.count()\n",
    "d = trimmed_net.agg({\"rt_times\" : \"sum\"}).collect()\n",
    "\n",
    "print(f\"\"\"Post filtering:\n",
    "\n",
    "Users: {a}\n",
    "RT'd accounts: {b}\n",
    "Total Links: {c}\n",
    "Total RTs: {d}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
